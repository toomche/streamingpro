# Run python project in distribute

When you do data pre-processing for machine learning, it's would be better sometimes than
writing SQL. The other situation is that the function you depends have no exists ET satisfy.


You python project should follow the project standard as mentioned in preview chapter,
that is to say, you should have MLproject and conda.yaml in your project. 

Normally, there are three steps when you write your python data pre-precessing program:   


1. read data file which generated by MLSQL Engine in specify directory of local disk
2. do what you want
3. write the result into the directory the MLSQL Engine tell you. Then MLSQL Engine 
will upload the result to HDFS automatically.


## Prerequisites

If you runs on yarn mode, please make sure you start the MLSQL Engine with follow configuration:

```

-streaming.ps.cluster.enable  should be  enabled.

Please make sure
1. you have the uber-jar of mlsql placed in --jars
2. Put mlsql-ps-service_xxx_2.11-xxx.jar to $SPARK_HOME/libs 

More detail about [mlsql-ps-service](https://github.com/allwefantasy/mlsql-ps-service)

Otherwise the executor will
fail to start and the whole application will fails.

```

## Steps

Notice that we can place our project in a HDFS path, or you can also write them 
in MLSQL script. Here, we will show you how to write them in MLSQL script.  

Let's create python script firstly 

```sql

set pythonScript='''
import os
import warnings
import sys

import mlsql

if __name__ == "__main__":
    warnings.filterwarnings("ignore")

    tempDataLocalPath = mlsql.internal_system_param["tempDataLocalPath"]

    isp = mlsql.params()["internalSystemParam"]
    tempModelLocalPath = isp["tempModelLocalPath"]
    if not os.path.exists(tempModelLocalPath):
        os.makedirs(tempModelLocalPath)
    with open(tempModelLocalPath + "/result.txt", "w") as f:
        f.write("jack")
''';
```

As this script shows, you can use `mlsql.internal_system_param["tempDataLocalPath"]` to get the data directory,
and call  `mlsql.internal_system_param["tempModelLocalPath"]` to get the target directory. 

Secondly, you should set your python environment:

```sql
set dependencies='''
name: tutorial
dependencies:
  - python=3.6
  - pip
  - pip:
    - --index-url https://mirrors.aliyun.com/pypi/simple/
    - numpy==1.14.3
    - kafka==1.3.5
    - pyspark==2.3.2
    - pandas==0.22.0
''';

load script.`dependencies` as dependencies;
select 1 as a as fakeTable;
run fakeTable as PythonEnvExt.`/tmp/jack` where condaFile="dependencies" and command="create";

```
Kafka and PySpark is required, because mlsql relies on them.


Thirdly,write the MLSQL script:

```sql
set modelPath="/tmp/jack2";

set data='''
{"jack":1}
''';

load jsonStr.`data` as testData;
load script.`pythonScript` as pythonScript;

run testData as RepartitionExt.`` where partitionNum="5" as newdata;    --partitionNum=5即将数据分成5个分区

run newdata as PythonParallelExt.`${modelPath}`
and scripts="pythonScript" 
and entryPoint="pythonScript"
and condaFile="dependencies";


```

If you have several scripts, just use concat them with ',' in parameter scripts. 
There should have five directories in `/tmp/jack2/model` and every directory have a json file(which you create in python)

you can add partitionKey to make the directory structure can be loaded in later steps:

```
and partitionKey="hp_date"
```

Now you can read them with follow command:

```
load json.`/tmp/jack2/model` as output;
```

How to deal with resource file? Try following commands:

```sql
run newdata as PythonParallelExt.`${modelPath}`
and scripts="pythonScript" 
and entryPoint="pythonScript"
and condaFile="dependencies"
and `fitParam.0.resource.a`="${HOME}/testStreamingB/testDirA"    -- 指定要加载文件目录
and `fitParam.0.resource.b`="${HOME}/testStreamingB/testDirB"    -- 指定要加载文件目录 
;
```

You can get them in your python script:

```python
    
dir_list_A = os.listdir(mlsql.internal_system_param["resource"]["a"])
dir_list_A = [i for i in dir_list_A if i.endswith(".txt")]
dir_list_B = os.listdir(mlsql.internal_system_param["resource"]["b"])
dir_list_B = [i for i in dir_list_B if i.endswith(".txt")]

```







